<!DOCTYPE html><html lang="en" mode="light" > <!-- The Head v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta property="og:image" content="/kernel/assets/img/neural-networks/logistic-regression/decision-boundary.png"><title>Logistic regression | Anarthal Kernel</title><meta name="generator" content="Jekyll v4.1.1" /><meta property="og:title" content="Logistic regression" /><meta name="author" content="anarthal" /><meta property="og:locale" content="en_US" /><meta name="description" content="An explanation of the math behind logistic regression with an example using a real-world dataset." /><meta property="og:description" content="An explanation of the math behind logistic regression with an example using a real-world dataset." /><link rel="canonical" href="https://anarthal.github.io/kernel/posts/logistic-regression/" /><meta property="og:url" content="https://anarthal.github.io/kernel/posts/logistic-regression/" /><meta property="og:site_name" content="Anarthal Kernel" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-07-08T00:00:00+02:00" /><meta name="google-site-verification" content="KNjn6jsPErqjkNHyg6KeODM3NUCCstBfD9OQgpT5lB4" /> <script type="application/ld+json"> {"@type":"BlogPosting","headline":"Logistic regression","url":"https://anarthal.github.io/kernel/posts/logistic-regression/","datePublished":"2020-07-08T00:00:00+02:00","dateModified":"2020-07-08T00:00:00+02:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://anarthal.github.io/kernel/posts/logistic-regression/"},"author":{"@type":"Person","name":"anarthal"},"description":"An explanation of the math behind logistic regression with an example using a real-world dataset.","@context":"https://schema.org"}</script> <!-- The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps Generated by: https://www.favicon-generator.org/ v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT license --><link rel="shortcut icon" href="/kernel/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/kernel/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/kernel/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/kernel/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/kernel/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/kernel/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/kernel/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/kernel/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/kernel/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/kernel/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/kernel/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/kernel/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/kernel/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/kernel/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/kernel/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/kernel/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/kernel/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/kernel/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/kernel/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/kernel/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="preload" as="style" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"> <!-- CSS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --><link rel="preload" as="style" href="/kernel/assets/css/post.css"><link rel="stylesheet" href="/kernel/assets/css/post.css"><link rel="preload" as="style" href="/kernel/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/kernel/assets/css/lib/bootstrap-toc.min.css" /><link rel="preload" as="script" href="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" async></script> <!-- JS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --> <script src="/kernel/assets/js/post.min.js" async></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script> <script src="/kernel/app.js" defer></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"> <!-- The Side Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/kernel/" alt="avatar"> <img src="/kernel/assets/img/sample/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/kernel/">Anarthal Kernel</a></div><div class="site-subtitle font-italic">A blog on data science and programming</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/kernel/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/kernel/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/kernel/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/kernel/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/kernel/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></li></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <a href="https://github.com/anarthal" target="_blank"> <i class="fab fa-github-alt"></i> </a> <a href="https://www.linkedin.com/in/ruben-perez-hidalgo" target="_blank"> <i class="fab fa-linkedin"></i> </a></div></div><!-- The Top Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/kernel/"> Posts </a> </span> <span>Logistic regression</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"> <!-- Fixed kramdown code highlight rendering: https://github.com/penibelst/jekyll-compress-html/issues/101 https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901 --><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Logistic regression</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <span class="timeago" data-toggle="tooltip" data-placement="bottom" title="Wed, Jul 8, 2020, 12:00 AM +0200"> Jul 8, 2020 <i class="unloaded">2020-07-08T00:00:00+02:00</i> </span> by <span class="author"> anarthal </span></div><div> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Wed, Aug 5, 2020, 5:37 PM +0200"> Aug 5, 2020 <i class="unloaded">2020-08-05T17:37:21+02:00</i> </span></div></div><div class="post-content"><p>In this post I will talk about one of the most basic models in Machine Learning: logistic regression. This post doesn’t assume any previous knowledge of logistic regression. Some prior knowledge on machine learning may be beneficial.</p><h2 id="introduction">Introduction</h2><p>Logistic regression is a machine learning model used to solve <a href="https://machinelearningmastery.com/types-of-classification-in-machine-learning/">classification</a> tasks. In a classification problem, we build a model that predicts the class label of an object, given its input features. In this post we will develop a model that predicts whether a board game is good or not given features like its complexity or number of players. Classification tasks like this, where the label has only two possible options, are called binary classification tasks. They are <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> problems.</p><p>Logistic regression is a <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">generalized linear model</a>. It is generalization of <a href="https://en.wikipedia.org/wiki/Linear_regression">linear regression</a>, which predictins an output variable as a linear combination of the input features (e.g. predicts the price of a house as a linear combination of its area, number of bedrooms…). It may be beneficial for the reader to have some knowledge on linear regression before reading this post, although it is not required.</p><p>In the following sections we will go through the basic maths behind logistic regression and work through an example of how to apply this model to binary classification.</p><h2 id="problem-statement">Problem statement</h2><p>Binary classification is the problem of predicting the class \(y \in \{0, 1\}\) of an object given a set of input features \(x \in \mathbb{R}^n\). To be able to train a model capable of making such predictions, we are given a set of correctly labeled examples: a set of points \((x^{(i)}, y^{(i)})\) with \(i \in [1, m]\).</p><p>As I am a big fan of board games, I will develop an example using <a href="https://www.kaggle.com/mrpantherson/board-game-data">this Kaggle dataset</a> on board game data. The dataset comprises 5000 real boardgames, together with features as their number of players, average game duration, number of people that have bought the game, and so on. The games are ranked from best to not-as-good using a rating defined by <a href="https://boardgamegeek.com/browse/boardgame">this website</a>. <a href="https://www.kaggle.com/anarthal/board-game-logistic-regression">This Kaggle kernel</a> contains the full code for the example we will develop. I will show some snippets here when relevant.</p><p>We will try to predict whether a board game is “top” or not, where we define a “top board game” as one being among the best 1000. Thus, \(y = 1\) if the game is one of the top 1000, and \(y = 0\) otherwise.</p><p>Our task is to build a model to predict y. For the sake of example, we will just use the two most relevant input features, as this will allow us to visualize the results better:</p><ul><li>\(x_1\) will be the <em>number of buyers</em> (<code class="language-plaintext highlighter-rouge">owned</code> in our dataset). As it seems logical, there is a strong positive correlation between the rating and the number of buyers: popular games are generally bought by more people than other games.</li><li>\(x_2\) will be <code class="language-plaintext highlighter-rouge">weight</code>, which measures how complex a game is, in a scale from 1.0 to 5.0. Games with higher <code class="language-plaintext highlighter-rouge">weight</code> have more complicated rules and more complex mechanics. There is a positive correlation between rating and <code class="language-plaintext highlighter-rouge">weight</code> (top-ranked games tend to be complex ones).</li></ul><p>In this case, we have than the number of training examples m = 5000 and the number of features n = 2. Each example \(x^{(i)} \in \mathbb{R}^2\).</p><h2 id="making-predictions">Making predictions</h2><p>As every machine learning algorithm, logistic regression has a number of parameters that will be learnt during training. Knowing the parameters and the input features, the model is able to predict the output variable y. So what are the parameters of logistic regression and how can we use them to output predictions?</p><p>It turns out that logistic regression is a linear model. Similarly to what linear regression does, we will compute the following linear combination of the input features for each example in the training set:</p>\[z = \theta_0 + \theta_1 x_1 + \theta_2 x_2\]<p>Where \(\theta_0\), \(\theta_1\) and \(\theta_2\) are the parameters of the logistic regression model.</p><p>However, z cannot be the output of our model, since we need to obtain a binary label, and \(z \in \mathbb{R}\). To obtain a prediction, we will apply the following function to z, known as the logistic or sigmoid function:</p>\[y_{prob} = \sigma(z) = \frac{1}{1 + e^{-z}}\]<p>If we plot the function, we get the following shape:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/kernel/assets/img/logistic-regression/sigmoid.png" alt="Sigmoid" /></p><ul><li>As we can see, \(y_{prob} \in (0, 1)\).</li><li>Big negative values of z output values close to zero.</li><li>Big positive values of z output values close to one.</li><li>Inputs close to zero output values close to 0.5.</li></ul><p>We can interpret this number as the probability that the given example belongs to the class \(y = 1\). So, if we perform this calculation for a certain example and obtain 0.1, that means that our model is convinced that this game is not top ranked. Conversely, if the had obtained 0.9, that would mean that our model is almost sure that the game is top ranked. We can take 0.5 as the boundary, such that we predict \(y = 0\) if \(y_{prob} &lt; 0.5\), and y = 1 otherwise.</p><p>In the more general case, we have n input features. We can represent each example as a column vector of features \(x \in \mathbb{R}^n\). We can also define a column vector containing all the parameters in our model, \(\theta \in \mathbb{R}^n\). With this notation, we can write:</p>\[z = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n = \theta_0 + \theta^T x\] \[y_{prob} = \sigma(z)\]<h2 id="training-the-model">Training the model</h2><p>As with every machine learning model, training a logistic regressor is just optimizing a cost function that tells the model how well it’s doing on the training set. For an individual training example (i), we will use the following function:</p>\[L(y^{(i)}, y_{prob}^{(i)}) = - y^{(i)}log(y_{prob}^{(i)}) - (1 - y^{(i)})log(1 - y_{prob}^{(i)})\]<p>Where \(y^{(i)}\) are the real labels of the examples (often called the ground truths) and \(y_{prob}^{(i)}\) are the probabilities output by our model. This function is called the <a href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html">log loss function</a>. It models our problem well because:</p><ul><li>If the actual label \(y^{(i)} = 0\), the first summand goes away, leaving the expression \(L(0, y_{prob}^{(i)}) = - log(1 - y_{prob})\), which is zero for \(y_{prob} = 0\) and tends to infinity for \(y_{prob} = 1\). Thus, if the actual label is zero, we are rewarding the model for outputing probabilities close to zero, and we are penalizing it for the opposite.</li><li>If the actual label \(y^{(i)} = 1\), the second summand goes away, leaving the expression \(L(1, y_{prob}^{(i)}) = - log(y_{prob})\), which is zero for \(y_{prob} = 1\) and tends to infinity for \(y_{prob} = 0\) (the opposite to the previous bullet point).</li></ul><p>This is for just one training example. The overall cost will be the average along all training examples:</p>\[J(\theta_0, \theta) = \frac{1}{m} \sum_{i=1}^{m} L(y^{(i)}, y_{prob}^{(i)}(\theta_0, \theta))\]<p>We can see that this function depends on the model parameters \(\theta_0\) and \(\theta\), as the predictions \(y_{prob}\) are computed using these. Training the model becomes into an optimization problem: finding the parameters \(\theta_0\) and \(\theta\) that makes the cost function J as small as possible. We can employ algorithms such as gradient descent to solve this problem.</p><h3 id="example-the-board-game-model">Example: the board game model</h3><p>Let’s use <code class="language-plaintext highlighter-rouge">sklearn</code> Python library to train a logistic regression model for our board game problem. First of all, some imports:</p><div class="language-py highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></table></code></div></div><p>We then load the data into a dataframe <code class="language-plaintext highlighter-rouge">df</code>. Our input features will be <code class="language-plaintext highlighter-rouge">owned</code> and <code class="language-plaintext highlighter-rouge">weight</code>, while the predicted variable will be <code class="language-plaintext highlighter-rouge">top</code>. I won’t show the code the read the data here. Feel free to check <a href="https://www.kaggle.com/anarthal/board-game-logistic-regression">the Kaggle kernel</a> if you are curious.</p><p>As usual in ML, we split our data into a train and a test set. We will use the first one to train the model, and the second one to evaluate its performance. We then <code class="language-plaintext highlighter-rouge">fit</code> our <code class="language-plaintext highlighter-rouge">LogisticRegression</code> object, which will solve the optimization problem described above:</p><div class="language-py highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s">'owned'</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">]],</span> <span class="n">df</span><span class="p">[</span><span class="s">'top'</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></table></code></div></div><p>Alright, model trained. What are the value of the learned parameters? We can access them using:</p><div class="language-py highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="n">theta0</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># theta0 = -5.76
</span><span class="n">theta1</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>    <span class="c1"># theta1 = 0.000726
</span><span class="n">theta2</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>    <span class="c1"># theta2 = 0.891
</span></pre></table></code></div></div><p>This means that, to make predictions, our model is computing:</p>\[y_{prob} = \sigma(0.000726 * owned + 0.891 * weight - 5.76)\]<h2 id="the-decision-boundary">The decision boundary</h2><p>As we are just using two features for predictions, it is easy to visualize them. The following figure shows the two features in the two axes. Points in green represent top board games (\(y = 1\)), while points in red represent the other games (\(y = 0\)):</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/kernel/assets/img/logistic-regression/features.png" alt="Features" /></p><p>As you can see, points in the lower left corner are much more likely to have \(y = 0\) than points in the middle, for example. To make predictions, our model is going to split the feature space into two regions. Examples contained in the first region will be predicted as positive, with the other ones as negative. The decision boundary is the frontier between the two. As we are in \(\mathbb{R}^2\) and the model is linear, the decision boundary will be a straight line:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/kernel/assets/img/logistic-regression/decision-boundary.png" alt="Decision boundary" /></p><p>The dashed black line is the decision boundary. The points in the red region are classified as negatives and the ones in the green region, as positives.</p><p>Let’s now visualize the predicted probability for each example.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/kernel/assets/img/logistic-regression/probabilities.png" alt="Probabilities" /></p><p>Our model has a clear opinion on examples far away from the decision boundary. However, things get blurrier when we approach the dashed line, where \(y_{prob} = 0.5\).</p><p>We talked earlier about the cost function, and how it penalizes the model making the wrong decisions. The following figure shows the cost associated to each example (the bigger the circle, the greater the cost):</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/kernel/assets/img/logistic-regression/costs.png" alt="Costs" /></p><p>As we mentioned earlier, misclassified examples are the ones incurring in greater cost. The further a misclassified point is from the decision boundary, the more sure our model is about making the wrong decision, and thus, the greater the cost. Fitting the model is equivalent to placing the dashed line in the position that minimizes the overall cost.</p><h2 id="evaluating-the-model-performance">Evaluating the model performance</h2><p>How good is our is our model at making predictions? Let’s use the test set to know it:</p><div class="language-py highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="c1"># Yields 0.9088
</span></pre></table></code></div></div><p>By default, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">LogisticRegression</a> uses accuracy as evaluation metric. Accuracy is defined as:</p>\[accuracy = \frac{Correctly\ classified\ examples}{Total\ examples}\]<p>That means that our model predicted 90.88% of the examples in the test set correctly. Not too bad for just using two features!</p><h2 id="conclusion">Conclusion</h2><p>This finishes our study of logistic regression. I hope the example has helped to clarify some of the maths behind the model.</p><p>A couple final thoughts:</p><ul><li>For the sake of example, we have just considered two features. There are plenty of other variables to improve our model.</li><li>The two employed features have very different scales. It may be beneficial for the model to normalize the variables, so they have a similar range and variance.</li><li>Another term is usually added to the cost function presented here, called the regularization term, which helps prevent the overfitting problem. As this is not a concern in a model as simple as ours, we have omitted it here.</li><li>We used accuracy for simplicity, but it may not be the best metric to choose, as the dataset is slightly imbalanced. Other metrics like ROC AUC or F1 score may be more adequate.</li></ul><p>Hope you have liked the post! Feedback and suggestions are always welcome.</p><h2 id="references">References</h2><ul><li>Kaggle dataset on board game data: <a href="https://www.kaggle.com/mrpantherson/board-game-data">https://www.kaggle.com/mrpantherson/board-game-data</a>.</li><li>Insights - Geek Board Game (Kaggle kernel): <a href="https://www.kaggle.com/devisangeetha/insights-geek-board-game">https://www.kaggle.com/devisangeetha/insights-geek-board-game</a>.</li><li>Board Game Geek: <a href="https://boardgamegeek.com/">https://boardgamegeek.com/</a>.</li><li>Machine Learning, Coursera course by Andrew Ng: <a href="https://www.coursera.org/learn/machine-learning/">https://www.coursera.org/learn/machine-learning/</a>.</li><li>Sklearn documentation: <a href="https://scikit-learn.org/stable/">https://scikit-learn.org/stable/</a></li><li><a href="https://machinelearningmastery.com/types-of-classification-in-machine-learning/">https://machinelearningmastery.com/types-of-classification-in-machine-learning/</a></li><li><a href="https://en.wikipedia.org/wiki/Supervised_learning">https://en.wikipedia.org/wiki/Supervised_learning</a></li></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/kernel/categories/data-science/'>Data Science</a>, <a href='/kernel/categories/machine-learning/'>Machine Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/kernel/tags/machinelearning/" class="post-tag no-text-decoration" >machinelearning</a> <a href="/kernel/tags/classification/" class="post-tag no-text-decoration" >classification</a> <a href="/kernel/tags/supervised/" class="post-tag no-text-decoration" >supervised</a> <a href="/kernel/tags/sklearn/" class="post-tag no-text-decoration" >sklearn</a> <a href="/kernel/tags/python/" class="post-tag no-text-decoration" >python</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><!-- Post sharing snippet v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://anarthal.github.io/kernel/posts/logistic-regression/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank"> <i class="fa-fw fab fa-linkedin"></i> </a> <a href="https://twitter.com/intent/tweet?text=Logistic regression - Anarthal Kernel&url=https://anarthal.github.io/kernel/posts/logistic-regression/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Logistic regression - Anarthal Kernel&u=https://anarthal.github.io/kernel/posts/logistic-regression/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Logistic regression - Anarthal Kernel&url=https://anarthal.github.io/kernel/posts/logistic-regression/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><!-- The Pannel on right side (Desktop views) v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><h3 data-toc-skip>Recent Update</h3><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/kernel/posts/underfitting-overfitting/">Underfitting, overfitting and model complexity</a></li><li><a href="/kernel/posts/neural-networks/">Deep dive into neural networks - the basics</a></li><li><a href="/kernel/posts/logistic-regression/">Logistic regression</a></li></ul></div><div id="access-tags"><h3 data-toc-skip>Trending Tags</h3><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/kernel/tags/machinelearning/">machinelearning</a> <a class="post-tag" href="/kernel/tags/classification/">classification</a> <a class="post-tag" href="/kernel/tags/supervised/">supervised</a> <a class="post-tag" href="/kernel/tags/sklearn/">sklearn</a> <a class="post-tag" href="/kernel/tags/python/">python</a> <a class="post-tag" href="/kernel/tags/deeplearning/">deeplearning</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><h3 data-toc-skip class="pl-3 pt-2 mb-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="post-extend-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <!-- Navigation buttons at the bottom of the post. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --><div class="post-navigation d-flex justify-content-between"> <span class="btn btn-outline-primary disabled"><p>-</p></span> <a href="/kernel/posts/underfitting-overfitting/" class="btn btn-outline-primary"><p>Underfitting, overfitting and model complexity</p></a></div><!-- The Disqus lazy loading. Powered by: https://osvaldas.info/lazy-loading-disqus-comments v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung MIT License --><div id="disqus" class="pt-2 pb-4"><p class="font-italic text-center text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/kernel/assets/js/lib/jquery.disqusloader.min.js"></script> <script> var options = { scriptUrl: '//anarthal-kernel.disqus.com/embed.js', disqusConfig: function() { this.page.url = 'https://anarthal.github.io/kernel/posts/logistic-regression/'; this.page.identifier = '/posts/logistic-regression/'; } }; $.disqusLoader('#disqus', options); </script> <!-- Recommend the other 3 posts according to the tags and categories of the current post, if the number is not enough, use the other latest posts to supplement. v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div id="related-posts" class="mt-4 mb-2 mb-sm-4 pb-2"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/kernel/posts/underfitting-overfitting/"><div class="card-body"> <span class="timeago small"> Jul 16, 2020 <i class="unloaded">2020-07-16T00:00:00+02:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Underfitting, overfitting and model complexity</h3><div class="text-muted small"><p>In this post I will talk about the underfitting and overfitting phenomena, and how model complexity affects them. I will also explain the bias-variance trade-off. We will demonstrate these concept...</p></div></div></a></div><div class="card"> <a href="/kernel/posts/neural-networks/"><div class="card-body"> <span class="timeago small"> Aug 5, 2020 <i class="unloaded">2020-08-05T00:00:00+02:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep dive into neural networks - the basics</h3><div class="text-muted small"><p>They are on everyone’s lips: every single post today seems to talk about deep neural networks and the bewildering variety of applications they are used for. Speech recognition, computer vision, nat...</p></div></div></a></div></div></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script> <!-- The Footer v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2020 <a href="https://github.com/anarthal">Ruben Perez</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/">Chirpy</a> theme.</p></div></div></footer></div><!-- The Search results v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/kernel/tags/machinelearning/">machinelearning</a> <a class="post-tag" href="/kernel/tags/classification/">classification</a> <a class="post-tag" href="/kernel/tags/supervised/">supervised</a> <a class="post-tag" href="/kernel/tags/sklearn/">sklearn</a> <a class="post-tag" href="/kernel/tags/python/">python</a> <a class="post-tag" href="/kernel/tags/deeplearning/">deeplearning</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <!-- The GA snippet v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-174570044-1', 'auto'); ga('send', 'pageview'); </script> <!-- Jekyll Simple Search loader v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/kernel/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://anarthal.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
