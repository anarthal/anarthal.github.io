<!DOCTYPE html><html lang="en" mode="light" > <!-- The Head v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta property="og:image" content="https://anarthal.github.io/kernel/assets/img/neural-networks/neural-networks-keras/handwritten-digits.png"><title>Deep dive into neural networks - Keras | Anarthal Kernel</title><meta name="generator" content="Jekyll v4.1.1" /><meta property="og:title" content="Deep dive into neural networks - Keras" /><meta name="author" content="anarthal" /><meta property="og:locale" content="en_US" /><meta name="description" content="Have you heard about Keras but never known what is it about or where to start with? In this post we will explain Keras basics and we will use it to build a handwritten digit classifier with a high level of accuracy! This is the last of a series of posts on neural networks. If you don’t know what terms like hidden unit or activation function mean, you may find it useful to read the first post of the series, on the basics of NNs. The problem presented here is a multiclass classification one, which the second post is all about." /><meta property="og:description" content="Have you heard about Keras but never known what is it about or where to start with? In this post we will explain Keras basics and we will use it to build a handwritten digit classifier with a high level of accuracy! This is the last of a series of posts on neural networks. If you don’t know what terms like hidden unit or activation function mean, you may find it useful to read the first post of the series, on the basics of NNs. The problem presented here is a multiclass classification one, which the second post is all about." /><link rel="canonical" href="https://anarthal.github.io/kernel/posts/neural-networks-keras/" /><meta property="og:url" content="https://anarthal.github.io/kernel/posts/neural-networks-keras/" /><meta property="og:site_name" content="Anarthal Kernel" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-10-06T00:00:00+02:00" /><meta name="google-site-verification" content="KNjn6jsPErqjkNHyg6KeODM3NUCCstBfD9OQgpT5lB4" /> <script type="application/ld+json"> {"mainEntityOfPage":{"@type":"WebPage","@id":"https://anarthal.github.io/kernel/posts/neural-networks-keras/"},"@type":"BlogPosting","url":"https://anarthal.github.io/kernel/posts/neural-networks-keras/","author":{"@type":"Person","name":"anarthal"},"headline":"Deep dive into neural networks - Keras","dateModified":"2020-10-06T00:00:00+02:00","datePublished":"2020-10-06T00:00:00+02:00","description":"Have you heard about Keras but never known what is it about or where to start with? In this post we will explain Keras basics and we will use it to build a handwritten digit classifier with a high level of accuracy! This is the last of a series of posts on neural networks. If you don’t know what terms like hidden unit or activation function mean, you may find it useful to read the first post of the series, on the basics of NNs. The problem presented here is a multiclass classification one, which the second post is all about.","@context":"https://schema.org"}</script> <!-- The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps Generated by: https://www.favicon-generator.org/ v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT license --><link rel="shortcut icon" href="/kernel/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/kernel/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/kernel/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/kernel/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/kernel/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/kernel/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/kernel/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/kernel/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/kernel/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/kernel/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/kernel/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/kernel/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/kernel/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/kernel/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/kernel/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/kernel/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/kernel/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/kernel/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/kernel/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/kernel/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="preload" as="style" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"> <!-- CSS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --><link rel="preload" as="style" href="/kernel/assets/css/post.css"><link rel="stylesheet" href="/kernel/assets/css/post.css"><link rel="preload" as="style" href="/kernel/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/kernel/assets/css/lib/bootstrap-toc.min.css" /><link rel="preload" as="script" href="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" async></script> <!-- JS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --> <script src="/kernel/assets/js/post.min.js" async></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script> <script src="/kernel/app.js" defer></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"> <!-- The Side Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/kernel/" alt="avatar"> <img src="/kernel/assets/img/sample/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/kernel/">Anarthal Kernel</a></div><div class="site-subtitle font-italic">A blog on data science and programming</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/kernel/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/kernel/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/kernel/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/kernel/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/kernel/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></li></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <a href="https://github.com/anarthal" target="_blank"> <i class="fab fa-github-alt"></i> </a> <a href="https://www.linkedin.com/in/ruben-perez-hidalgo" target="_blank"> <i class="fab fa-linkedin"></i> </a></div></div><!-- The Top Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/kernel/"> Posts </a> </span> <span>Deep dive into neural networks - Keras</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"> <!-- Fixed kramdown code highlight rendering: https://github.com/penibelst/jekyll-compress-html/issues/101 https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901 --><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Deep dive into neural networks - Keras</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <span class="timeago" data-toggle="tooltip" data-placement="bottom" title="Tue, Oct 6, 2020, 12:00 AM +0200"> Oct 6, 2020 <i class="unloaded">2020-10-06T00:00:00+02:00</i> </span> by <span class="author"> anarthal </span></div><div> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Wed, Oct 7, 2020, 10:35 AM +0200"> Oct 7, 2020 <i class="unloaded">2020-10-07T10:35:32+02:00</i> </span></div></div><div class="post-content"><p>Have you heard about Keras but never known what is it about or where to start with? In this post we will explain Keras basics and we will use it to build a handwritten digit classifier with a high level of accuracy! This is the last of a series of posts on neural networks. If you don’t know what terms like <em>hidden unit</em> or <em>activation function</em> mean, you may find it useful to read <a href="/kernel/posts/neural-networks/">the first post of the series</a>, on the basics of NNs. The problem presented here is a multiclass classification one, which the <a href="/kernel/posts/neural-networks-multiclass/">second post</a> is all about.</p><p><strong>TL; DR</strong>: code for this tutorial is <a href="https://www.kaggle.com/anarthal/mnist-digit-recognition-plain-network">here</a>.</p><h1 id="problem-statement">Problem statement</h1><p>Imagine you’re working for the post service. Packets are routed using their postcode. You would like to come up with a model that knows how to read the handwritten numbers in the envelope postcodes, so you can automate the entire process. The problem of identifying a digit given an image is called handwritten digit recognition.</p><p>We will be working with the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST database of handwritten digits</a>, which is the de-facto “hello world” dataset for computer vision applications. It consists of a set of 28 by 28 grayscale images containing handwritten digits. Each image is labeled from 0 to 9, according to the digit it represents. This is what the dataset looks like:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/kernel/assets/img/neural-networks-keras/handwritten-digits.png" alt="Handwritten digits" /></p><p>Our task is to build a model that classifies as much images correctly as it can. This is a <a href="https://en.wikipedia.org/wiki/Multiclass_classification">multiclass classification</a> problem, as there are 10 possible classes an image may belong to. We will be using accuracy to measure the classifier’s performance.</p><p>The MNIST dataset is available in Kaggle through <a href="https://www.kaggle.com/c/digit-recognizer">this competition</a>. I will show some code snippets throughout this post; you can find the entire code listing for it in <a href="https://www.kaggle.com/anarthal/mnist-digit-recognition-plain-network">this Kaggle kernel</a>.</p><h1 id="network-architecture">Network architecture</h1><p>We will implement a fully connected neural network architecture like the one shown in the image.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/kernel/assets/img/neural-networks-keras/network-architecture.png" alt="Network architecture" /></p><p>Each image has \(\text{28 x 28} = 784\) grayscale pixel intensities. That means each image can be described as a set of 784 real numbers. We will simply use these as input features for our network. The output layer will have 10 units, as there are 10 possible classes in our classification problem, and will use the softmax activation function. The network will output a prediction vector \(y_{prob} \in \mathbb{R}^{10}\), each element representing the probability that the given image is a certain digit (e.g. \(y_{prob}[2]\) represents the probability that the input image is the digit 2). We will employ cross-entropy as the loss function. If you feel unsure about any of these concepts, feel free to check <a href="/kernel/posts/neural-networks-multiclass/">the previous post</a> on neural networks for multiclass classification.</p><h1 id="keras-and-tensorflow">Keras and Tensorflow</h1><p>Okay, we now know what we want to build. Let’s see how to build it. We will use the <a href="https://keras.io/">Keras</a> framework, which makes it easy to define models like the one presented above.</p><p>If you’re familiar with the current deep learning landscape, you probably have heard both of Keras and Tensorflow. And you may be confused about the relationship between them: what is the difference between them? Is Keras part of Tensorflow? Does Keras use Tensorflow?</p><p>Keras is a library to build neural network models, and tries to be as simple as it can. On the other hand, <a href="https://www.tensorflow.org/">Tensorflow</a> is a symbolic math library which allows creating arbitrary machine learning models. Keras is higher level (and thus simpler) than Tensorflow. Actually, Keras uses Tensorflow. We will be using just Keras because our model is simple enough to be directly supported by the Keras APIs.</p><p>As of Keras 2.3.0, <code class="language-plaintext highlighter-rouge">keras</code> is included in the <code class="language-plaintext highlighter-rouge">tensorflow</code> Python package. This has not always been the case; if you are interested to know more about the history of these two libraries, check <a href="https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/">this post</a> out.</p><p>With that in mind, let’s do the import:</p><div class="language-py highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
</pre></table></code></div></div><p>When building a model with Keras, you should perform these three steps:</p><ol><li><strong>Defining the network architecture</strong>. This is writing in code the model architecture we explained before.</li><li><strong>Compiling the model</strong>. At this point we specify the loss function, and other parameters of the training process, like the optimizer.</li><li><strong>Fitting the model</strong>. Here we actually pass in the data so Keras can train the network.</li></ol><p>We will go through them in the next sections.</p><h1 id="defining-the-architecture">Defining the architecture</h1><p>We will be using the <a href="https://keras.io/api/models/sequential/"><code class="language-plaintext highlighter-rouge">Sequential</code></a> class, which is the easiest when we have a simple networks like ours. The model definition looks like the following:</p><div class="language-py highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="n">lambda_</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,)),</span>
    <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'l1'</span><span class="p">,</span>
    		           <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">regularizers</span><span class="p">.</span><span class="n">l2</span><span class="p">(</span><span class="n">lambda_</span><span class="p">)),</span>
    <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'l2'</span><span class="p">,</span>
    		           <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">regularizers</span><span class="p">.</span><span class="n">l2</span><span class="p">(</span><span class="n">lambda_</span><span class="p">)),</span>
    <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'output'</span><span class="p">,</span>
    		           <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">regularizers</span><span class="p">.</span><span class="n">l2</span><span class="p">(</span><span class="n">lambda_</span><span class="p">))</span>
<span class="p">])</span>
</pre></table></code></div></div><ul><li>The <a href="https://keras.io/api/models/sequential/"><code class="language-plaintext highlighter-rouge">Sequential</code></a> class takes as input a list of layers and connects them in sequential order. This class is a Keras <a href="https://keras.io/api/models/model/">Model</a>, which means it offers operations like <code class="language-plaintext highlighter-rouge">compile()</code>, <code class="language-plaintext highlighter-rouge">fit()</code> and <code class="language-plaintext highlighter-rouge">predict()</code>.</li><li>The <a href="https://keras.io/api/layers/core_layers/input/"><code class="language-plaintext highlighter-rouge">Input</code></a> object represents the input layer. We pass in a tuple indicating the shape of the input feature vector. Remember that we have a feature per pixel, and that yields 784 features. Note that we do not have to specify how many samples we have.</li><li>We use <a href="https://keras.io/api/layers/core_layers/dense/"><code class="language-plaintext highlighter-rouge">Dense</code></a> objects to represent fully connected layers. The first parameter is the number of hidden units. The first two layers use the ReLU activation function, while the output layer uses softmax. The <code class="language-plaintext highlighter-rouge">name</code> parameter is optional and is used for display and debugging purposes only.</li><li>We have included <a href="https://keras.io/api/layers/regularizers/">layer weight regularizers</a> in all the layers. <a href="https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a">Regularization</a> is a technique to prevent overfitting. The higher the <code class="language-plaintext highlighter-rouge">lambda_</code> parameter, the stronger the regularizer effect is.</li></ul><p>Note that the alternative to the sequential class is the <a href="https://keras.io/guides/functional_api/">functional API</a>, which you can employ for more complex architectures.</p><h1 id="compiling-the-model">Compiling the model</h1><p>Let’s dive into the second step:</p><div class="language-py highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(),</span>
    <span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">]</span>
<span class="p">)</span>
</pre></table></code></div></div><p>This is telling Keras additional information about how to perform the training process:</p><ul><li>The <a href="https://keras.io/api/optimizers/adam/">Adam</a> optimizer should be used. The optimizer is the algorithm that solves the minimization problem that training presents: given a labeled training set, it tries to find the set of weights that make the loss function as small as possible. You may have heard of <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>, as it is the most essential optimizer used in deep learning. Adam is a modified version of gradient descent that usually works better. <a href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/">This post</a> explores the algorithm in depth.</li><li>It should use the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy">categorical cross-entropy</a> loss function. Check <a href="/kernel/posts/neural-networks-multiclass/#the-loss-function">the previous post</a> if you are not familiar with this loss function.</li><li>During the training process, Keras will track the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy">accuracy metric</a>. This means it will record the evolution of this score as the network is trained, both for training and validation data. By looking at this information we can get insights like if we are training the network long enough or not. You can track as many metrics as you want. A comprehensive list of available metrics is <a href="https://keras.io/api/metrics/">here</a>.</li></ul><h1 id="fitting-the-model">Fitting the model</h1><p>Finally, let’s tell Keras to train our network. Let’s say we have 42000 training examples:</p><div class="language-py highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="n">X_train</span> <span class="o">=</span> <span class="c1"># ... read input data; X_train.shape is (42000, 784)
</span><span class="n">labels</span> <span class="o">=</span>  <span class="c1"># ... read input data; labels.shape is (42000, 1)
</span>          <span class="c1">#     and each value is between 0 and 9
</span>
<span class="c1"># convert to one-hot, y_train.shape is (42000, 10)
</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> 

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span> 
    <span class="n">y_train</span><span class="p">,</span> 
    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> 
    <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span>
<span class="p">)</span>
</pre></table></code></div></div><ul><li>We have converted our numeric <code class="language-plaintext highlighter-rouge">labels</code> to a one-hot encoding, using the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical"><code class="language-plaintext highlighter-rouge">to_categorical</code></a> helper function. Recall that in this one-hot encoding scheme, every label is represented by a vector of 10 elements, one per possible class.</li><li><code class="language-plaintext highlighter-rouge">validation_split=0.2</code> tells Keras to perform a train-validation split before fitting the data. Keras will use 80% of the training examples to actually train the network, leaving the remaining 20% out for validation, so we can have unbiased estimates of our metrics.</li><li><code class="language-plaintext highlighter-rouge">batch_size</code> sets the mini-batch size. Mini-batching is a technique that makes the optimization process quicker. The cost function is usually defined as the average over all the training set of the loss function. If you have a big training set, computing the cost function on the entire training set may be very expensive. Instead, optimization algorithms usually work in mini-batches: they first look into a small subset of the data and make some progress towards the minimum. They then jump into the next mini-batch, repeating the process until they pass through all the training set. This allows for faster progress. <a href="https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a">This article</a> explores the topic of mini-batches for gradient descent, and is extrapolable to the Adam optimization algorithm.</li><li><code class="language-plaintext highlighter-rouge">epochs</code> controls how long the network should be trained for. An <strong>epoch</strong> is a pass of the optimization algorithm through the entire training set. Setting <code class="language-plaintext highlighter-rouge">epochs=50</code> means that the algorithm will stop after 50 passes. You should specify a number of epochs big enough such that further training yields no significant gain in performance. We can use the tracked metrics to verify this.</li><li><code class="language-plaintext highlighter-rouge">model.fit()</code> returns a history object, containing the tracked metrics. We will explore it further in the next section.</li></ul><h1 id="evaluating-the-model">Evaluating the model</h1><p>By providing validation data with <code class="language-plaintext highlighter-rouge">validation_split</code>, Keras performs model validation for us while training. By looking at the logs, I’m getting 96.48% accuracy on the train set and a 94.63% on the validation set, which is quite good for a simple network like ours. Note that the decimals may vary for you due to random initialization of weights. Train and validation scores are quite close, so it doesn’t seem like our model is overfitting a lot.</p><p>Would our model improve if we trained it further? We can examine the returned <code class="language-plaintext highlighter-rouge">history</code> object to answer this question:</p><div class="language-py highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">]</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_accuracy'</span><span class="p">]</span>
</pre></table></code></div></div><p>Note that:</p><ul><li><code class="language-plaintext highlighter-rouge">history</code> is a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History">keras.callbacks.History</a> object.</li><li><code class="language-plaintext highlighter-rouge">history.history</code> is a Python <code class="language-plaintext highlighter-rouge">dict</code> containing the recorded values for the loss and each defined metric.</li><li>Each of the four items are Python lists with an element per epoch. In our case, the lists contain 50 elements.</li><li><code class="language-plaintext highlighter-rouge">loss</code> is the value of the loss function for the training set; <code class="language-plaintext highlighter-rouge">val_loss</code> is the loss function for the validation set.</li><li>The same applies for <code class="language-plaintext highlighter-rouge">accuracy</code> and <code class="language-plaintext highlighter-rouge">val_accuracy</code>.</li></ul><p>If we plot these measures against the epoch number, we get the following:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/kernel/assets/img/neural-networks-keras/history.png" alt="History" /></p><p>We can see that the loss goes down sharply at first and then flattens. The accuracy shows the reverse tendency. After 50 epochs, the performance seems almost flat. Thus, training the network longer won’t yield much better results. We can also see that both train and validation scores are quite close, which indicates we are not overfitting heavily.</p><h1 id="making-predictions">Making predictions</h1><p>You can make predictions for a bunch of examples:</p><div class="language-py highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">preds_prob</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">preds_label</span> <span class="o">=</span> <span class="n">preds_prob</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></table></code></div></div><p>Where <code class="language-plaintext highlighter-rouge">X_test</code> has the same columns as <code class="language-plaintext highlighter-rouge">X_train</code>. <code class="language-plaintext highlighter-rouge">predict()</code> returns an array of probabilities, with 10 columns, one for each class. We can transform that into a label using numpy’s <a href="https://numpy.org/doc/stable/reference/generated/numpy.argmax.html"><code class="language-plaintext highlighter-rouge">argmax</code></a> in each row (<code class="language-plaintext highlighter-rouge">axis=1</code>).</p><p>Finally, let’s inspect both a correct and a wrong prediction:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/kernel/assets/img/neural-networks-keras/predictions.png" alt="Predictions" /></p><h1 id="conclusion">Conclusion</h1><p>This concludes our three-post series on neural networks! Thanks for reading this far. As always, please share, and feedback is always welcome!</p><h1 id="references">References</h1><ul><li>Deep Learning Specialization, Coursera courses by Andrew Ng: <a href="https://www.coursera.org/specializations/deep-learning">https://www.coursera.org/specializations/deep-learning</a>.</li><li>How to attack a machine learning model?, Kaggle kernel by Laura Fink: <a href="https://www.kaggle.com/allunia/how-to-attack-a-machine-learning-model">https://www.kaggle.com/allunia/how-to-attack-a-machine-learning-model</a>.</li><li>Digit Recognizer, Kaggle competition: <a href="https://www.kaggle.com/c/digit-recognizer">https://www.kaggle.com/c/digit-recognizer</a>.</li><li>NN-SVG, a tool to draw neural network architectures by Alexander Lenail: <a href="http://alexlenail.me/NN-SVG/index.html">http://alexlenail.me/NN-SVG/index.html</a>.</li><li>Keras vs. tf.keras: What’s the difference in TensorFlow 2.0?, by Adrian Rosebrock: <a href="https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/">https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/</a></li><li>Regularization in Machine Learning, by Prashant Gupta: <a href="https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a">https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a</a>.</li><li>Gentle Introduction to the Adam Optimization Algorithm for Deep Learning, by Jason Brownlee: <a href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/">https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/</a>.</li><li>Batch, Mini Batch &amp; Stochastic Gradient Descent, by Sushant Patrikar: <a href="https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a">https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a</a>.</li><li>Keras official documentation: <a href="https://keras.io/">https://keras.io/</a>.</li></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/kernel/categories/data-science/'>Data Science</a>, <a href='/kernel/categories/machine-learning/'>Machine Learning</a>, <a href='/kernel/categories/deep-learning/'>Deep Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/kernel/tags/machinelearning/" class="post-tag no-text-decoration" >machinelearning</a> <a href="/kernel/tags/deeplearning/" class="post-tag no-text-decoration" >deeplearning</a> <a href="/kernel/tags/classification/" class="post-tag no-text-decoration" >classification</a> <a href="/kernel/tags/python/" class="post-tag no-text-decoration" >python</a> <a href="/kernel/tags/keras/" class="post-tag no-text-decoration" >keras</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><!-- Post sharing snippet v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://anarthal.github.io/kernel/posts/neural-networks-keras/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank"> <i class="fa-fw fab fa-linkedin"></i> </a> <a href="https://twitter.com/intent/tweet?text=Deep dive into neural networks - Keras - Anarthal Kernel&url=https://anarthal.github.io/kernel/posts/neural-networks-keras/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Deep dive into neural networks - Keras - Anarthal Kernel&u=https://anarthal.github.io/kernel/posts/neural-networks-keras/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Deep dive into neural networks - Keras - Anarthal Kernel&url=https://anarthal.github.io/kernel/posts/neural-networks-keras/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><!-- The Pannel on right side (Desktop views) v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><h3 data-toc-skip>Recent Update</h3><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/kernel/posts/neural-networks-keras/">Deep dive into neural networks - Keras</a></li><li><a href="/kernel/posts/neural-networks/">Deep dive into neural networks - the basics</a></li><li><a href="/kernel/posts/underfitting-overfitting/">Underfitting, overfitting and model complexity</a></li><li><a href="/kernel/posts/logistic-regression/">Logistic regression</a></li></ul></div><div id="access-tags"><h3 data-toc-skip>Trending Tags</h3><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/kernel/tags/machinelearning/">machinelearning</a> <a class="post-tag" href="/kernel/tags/classification/">classification</a> <a class="post-tag" href="/kernel/tags/python/">python</a> <a class="post-tag" href="/kernel/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/kernel/tags/supervised/">supervised</a> <a class="post-tag" href="/kernel/tags/sklearn/">sklearn</a> <a class="post-tag" href="/kernel/tags/keras/">keras</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><h3 data-toc-skip class="pl-3 pt-2 mb-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="post-extend-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <!-- Navigation buttons at the bottom of the post. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --><div class="post-navigation d-flex justify-content-between"> <a href="/kernel/posts/neural-networks-multiclass/" class="btn btn-outline-primary"><p>Deep dive into neural networks - multiclass classification</p></a> <span class="btn btn-outline-primary disabled"><p>-</p></span></div><!-- The Disqus lazy loading. Powered by: https://osvaldas.info/lazy-loading-disqus-comments v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung MIT License --><div id="disqus" class="pt-2 pb-4"><p class="font-italic text-center text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/kernel/assets/js/lib/jquery.disqusloader.min.js"></script> <script> var options = { scriptUrl: '//anarthal-kernel.disqus.com/embed.js', disqusConfig: function() { this.page.url = 'https://anarthal.github.io/kernel/posts/neural-networks-keras/'; this.page.identifier = '/posts/neural-networks-keras/'; } }; $.disqusLoader('#disqus', options); </script> <!-- Recommend the other 3 posts according to the tags and categories of the current post, if the number is not enough, use the other latest posts to supplement. v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div id="related-posts" class="mt-4 mb-2 mb-sm-4 pb-2"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/kernel/posts/neural-networks/"><div class="card-body"> <span class="timeago small"> Aug 5, 2020 <i class="unloaded">2020-08-05T00:00:00+02:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep dive into neural networks - the basics</h3><div class="text-muted small"><p>They are on everyone’s lips: every single post today seems to talk about deep neural networks and the bewildering variety of applications they are used for. Speech recognition, computer vision, nat...</p></div></div></a></div><div class="card"> <a href="/kernel/posts/neural-networks-multiclass/"><div class="card-body"> <span class="timeago small"> Aug 23, 2020 <i class="unloaded">2020-08-23T00:00:00+02:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep dive into neural networks - multiclass classification</h3><div class="text-muted small"><p>Some machine learning problems involve classifying an object into one of N classes. These are called multiclass classification problems, as opposed to binary classification, where there is just a p...</p></div></div></a></div><div class="card"> <a href="/kernel/posts/logistic-regression/"><div class="card-body"> <span class="timeago small"> Jul 8, 2020 <i class="unloaded">2020-07-08T00:00:00+02:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Logistic regression</h3><div class="text-muted small"><p>In this post I will talk about one of the most basic models in Machine Learning: logistic regression. This post doesn’t assume any previous knowledge of logistic regression. Some prior knowledge on...</p></div></div></a></div></div></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script> <!-- The Footer v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2020 <a href="https://github.com/anarthal">Ruben Perez</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/">Chirpy</a> theme.</p></div></div></footer></div><!-- The Search results v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/kernel/tags/machinelearning/">machinelearning</a> <a class="post-tag" href="/kernel/tags/classification/">classification</a> <a class="post-tag" href="/kernel/tags/python/">python</a> <a class="post-tag" href="/kernel/tags/deeplearning/">deeplearning</a> <a class="post-tag" href="/kernel/tags/supervised/">supervised</a> <a class="post-tag" href="/kernel/tags/sklearn/">sklearn</a> <a class="post-tag" href="/kernel/tags/keras/">keras</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <!-- The GA snippet v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-174570044-1', 'auto'); ga('send', 'pageview'); </script> <!-- Jekyll Simple Search loader v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/kernel/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://anarthal.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
