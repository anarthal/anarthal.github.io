<!DOCTYPE html><html lang="en" mode="light" > <!-- The Head v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta property="og:image" content="/kernel/assets/img/neural-networks/neural-networks/layers.png"><title>Deep dive into neural networks - the basics | Anarthal Kernel</title><meta name="generator" content="Jekyll v4.1.1" /><meta property="og:title" content="Deep dive into neural networks - the basics" /><meta name="author" content="anarthal" /><meta property="og:locale" content="en_US" /><meta name="description" content="This post covers what are neural networks and how they make predictions for binary classification problems." /><meta property="og:description" content="This post covers what are neural networks and how they make predictions for binary classification problems." /><link rel="canonical" href="https://anarthal.github.io/kernel/posts/neural-networks/" /><meta property="og:url" content="https://anarthal.github.io/kernel/posts/neural-networks/" /><meta property="og:site_name" content="Anarthal Kernel" /><meta property="og:image" content="https://anarthal.github.io/kernel/neural-networks/layers.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-08-05T00:00:00+02:00" /><meta name="google-site-verification" content="KNjn6jsPErqjkNHyg6KeODM3NUCCstBfD9OQgpT5lB4" /> <script type="application/ld+json"> {"@type":"BlogPosting","headline":"Deep dive into neural networks - the basics","url":"https://anarthal.github.io/kernel/posts/neural-networks/","datePublished":"2020-08-05T00:00:00+02:00","dateModified":"2020-08-05T00:00:00+02:00","image":"https://anarthal.github.io/kernel/neural-networks/layers.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://anarthal.github.io/kernel/posts/neural-networks/"},"author":{"@type":"Person","name":"anarthal"},"description":"This post covers what are neural networks and how they make predictions for binary classification problems.","@context":"https://schema.org"}</script> <!-- The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps Generated by: https://www.favicon-generator.org/ v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT license --><link rel="shortcut icon" href="/kernel/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/kernel/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/kernel/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/kernel/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/kernel/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/kernel/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/kernel/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/kernel/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/kernel/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/kernel/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/kernel/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/kernel/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/kernel/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/kernel/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/kernel/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/kernel/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/kernel/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/kernel/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/kernel/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/kernel/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="preload" as="style" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"> <!-- CSS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --><link rel="preload" as="style" href="/kernel/assets/css/post.css"><link rel="stylesheet" href="/kernel/assets/css/post.css"><link rel="preload" as="style" href="/kernel/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/kernel/assets/css/lib/bootstrap-toc.min.css" /><link rel="preload" as="script" href="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" async></script> <!-- JS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --> <script src="/kernel/assets/js/post.min.js" async></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script> <script src="/kernel/app.js" defer></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"> <!-- The Side Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/kernel/" alt="avatar"> <img src="/kernel/assets/img/sample/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/kernel/">Anarthal Kernel</a></div><div class="site-subtitle font-italic">A blog on data science and programming</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/kernel/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/kernel/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/kernel/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/kernel/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/kernel/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></li></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <a href="https://github.com/anarthal" target="_blank"> <i class="fab fa-github-alt"></i> </a> <a href="https://www.linkedin.com/in/ruben-perez-hidalgo" target="_blank"> <i class="fab fa-linkedin"></i> </a></div></div><!-- The Top Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/kernel/"> Posts </a> </span> <span>Deep dive into neural networks - the basics</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"> <!-- Fixed kramdown code highlight rendering: https://github.com/penibelst/jekyll-compress-html/issues/101 https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901 --><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Deep dive into neural networks - the basics</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <span class="timeago" data-toggle="tooltip" data-placement="bottom" title="Wed, Aug 5, 2020, 12:00 AM +0200"> Aug 5, 2020 <i class="unloaded">2020-08-05T00:00:00+02:00</i> </span> by <span class="author"> anarthal </span></div></div><div class="post-content"> <img src="neural-networks/layers.png"><p>They are on everyone’s lips: every single post today seems to talk about deep neural networks and the bewildering variety of applications they are used for. <a href="https://en.wikipedia.org/wiki/Speech_recognition">Speech recognition</a>, <a href="https://en.wikipedia.org/wiki/Computer_vision">computer vision</a>, <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>… the possibilities seem endless. Neural networks are the workhorse of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a>, a subset of machine learning. But what is a neural network? How does it work?</p><p>This is the first of a series of posts on neural networks. Our ultimate goal will be to build a network able to recognize handwritten digits, as per the <a href="https://www.kaggle.com/c/digit-recognizer">MNIST dataset</a>. This first post presents what an artificial neural network is and the basic maths behind it. I’ve followed the same notation as Andrew Ng in his awesome <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning specialization</a> on Coursera.</p><p>To understand this post you should have basic notions of machine learning, linear algebra and calculus. In particular, you should already know what a classification problem is and how a logistic regression model can help. If you are not familiar with these concepts, <a href="/kernel/posts/logistic-regression/">this blog post</a> may help. You should also know how to multiply matrices and what a derivative is.</p><p>Enough talk. Let’s dive deep into deep learning (pun intended)!</p><h2 id="what-is-a-neural-network">What is a neural network?</h2><p>If you are super-hyped, expecting a definition that compares a neural network with human brain, you’re out of luck. A neural network is a machine learning model, just like <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> or <a href="https://xgboost.readthedocs.io/en/latest/">XGBoost</a>. Like these models, neural networks can be used for tasks like classification and regression.</p><p>What is the difference between traditional models and neural networks then? The latter are much more complex models. They may have billions of parameters, which allows them to learn really complex functions. This makes them suitable for incredible applications, like detecting objects (e.g. cars) in an image or diagnosing lung cancer given a radiography.</p><p>There are many types of neural networks depending on the field of application. <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional neural networks</a> and <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks</a> are used in computer vision and natural language processing, respectively. In this post we will focus on <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">artificial neural networks</a>, often called <em>fully connected</em> networks or just <em>neural networks</em>, for short. These are the basic building block on which the others are based. It is important to understand these well before jumping into the others.</p><h2 id="when-to-use-a-neural-network">When to use a neural network</h2><p>Before anyone gets too excited, a word of warning: as with every tool, it is adequate for some scenarios and it is not for other ones. If you try to apply neural networks to every single problem, chances are you will end up wasting your time.</p><p>Neural networks are complex models, and thus are adequate for complex problems (reading <a href="/kernel/posts/underfitting-overfitting/">my post on model complexity</a> may help understand). Applications like speech recognition, computer vision or natural language processing are usually the niche use for neural networks. If you have a simple classification problem with a couple plain numeric variables, chances are that <a href="https://xgboost.readthedocs.io/en/latest/">XGBoost</a> or <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">random forest</a> will work better.</p><p>As with any complex model, neural networks are prone to overfitting if you do not feed them with enough data. If you have very little data to solve your problem, neural networks aren’t likely to be very effective.</p><p>If you are interested, <a href="https://medium.com/datadriveninvestor/when-not-to-use-neural-networks-89fb50622429">this post</a> explores this topic in depth.</p><h2 id="logistic-regression-as-a-mini-neural-network">Logistic regression as a mini neural network</h2><p>Let’s get ourhands dirty! How does a neural network work? How does it make predictions? It turns out that neural networks can be seen as a generalization of logistic regression. Let’s review the latter from a different angle before jumping into full-blown neural networks.</p><p>Let’s say we are trying to solve a binary classification problem using logistic regression (like <a href="/kernel/posts/logistic-regression/">this one</a>). We are given the input features \(x_1, x_2... x_n\) and we are asked to predict the target label \(y \in \{0, 1\}\). This is what logistic regression would do:</p><ul><li>Compute a linear function of the inputs.</li><li>Apply the sigmoid non-linear function to generate the prediction.</li></ul>\[z = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b\] \[y_{prob} = a = \sigma(z)\]<p>Where \(w_i\) are the <strong>weights</strong> of the logistic regression model and \(b\) is the <strong>bias term</strong> (if you read <a href="/kernel/posts/logistic-regression/">this</a>, I called them \(\theta_i\) and \(\theta_0\), respectively). These are the parameters that the model learns by minimizing the <em>cost function</em>.</p><p>The sigmoid function makes the output have a non-linear relationship with the input. In the neural network context, these non-linear functions are called <strong>activation functions</strong>.</p><p>This simple logistic regression model can be thought of as a computation graph:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/kernel/assets/img/neural-networks/logistic-unit.jpg" alt="Logistic unit" /></p><p>What does this have to do with neural networks? ANNs are composed of lots of <strong>units</strong> like the one shown above, interconnected to each other.</p><h2 id="anatomy-of-a-neural-network-layers">Anatomy of a neural network: layers</h2><p>The idea of a neural network is to connect several units like the one above together, such that the output of one unit is wired to the input of another. Units are grouped in <strong>layers</strong>, forming a structure like the following:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/kernel/assets/img/neural-networks/layers.png" alt="Layers" /></p><p>Don’t get intimidated by the notation in this diagram! We will go through it in a minute.</p><p>As shown above, there are three types of layers:</p><ul><li>The <strong>input layer</strong> is shown in blue, and it is always the first one. It does no computation: it is just a way to represent the input to our network (the features \(x_i\)).</li><li>The <strong>hidden layers</strong>, shown in green, perform the computation explained before: a linear function followed by a non-linear activation function. The first hidden layer gets the features as input, while deeper layers get the output of previous layer’s units.</li><li>The <strong>output layer</strong> is the last one, and also performs a similar computation as hidden layers. For binary classification, the output layer has a single unit, and its result is the prediction of the network.</li></ul><p>As the input layer does not perform any computation, we don’t count it as an actual layer. Thus, the above network has 3 layers. In general, a network may have \(L\) layers ( \(L = 3\) in this case).</p><p>Note that each unit is connected to every single unit in the previous layer. This is why this network architecture is sometime called <em>fully connected</em>.</p><p>The result of the computation performed by a unit is called the <strong>activation</strong> for that unit. In the next section we will see how these values are computed.</p><h2 id="computing-activations">Computing activations</h2><p>We will denote by \(a_i^{[l]}\) the activation of the \(i\)th unit in layer number \(l\). For example, \(a_3^{[1]}\) is the output of the 3rd unit of the first layer. Look at the figure in the previous section to double-check that you understand the notation. Thus, the text inside each unit repesents its output.</p><p>To compute the activations we must follow the same steps as in logistic regression. For the sake of example, let’s compute \(a_1^{[2]}\), the activation of the first unit in the second layer:</p>\[z_1^{[2]} = w_{21}^{[2]} a_1^{[1]} + w_{22}^{[2]} a_2^{[1]} + w_{23}^{[2]} a_3^{[1]} + w_{24}^{[2]} a_4^{[1]} + b_1^{[2]}\] \[a_1^{[2]} = g^{[2]}(z_1^{[2]}) = \sigma(z_1^{[2]})\]<p>Wow, that seems intimidating. Don’t get fooled by this apparently complex expression: it is just the same as logistic regression! The only difference is that different units may have different values for the weights and the bias term. We are using the following notation:</p><ul><li>\(w_{ij}^{[l]}\) is the weight that unit \(j\) in layer \(l\) gives to the activation coming from the \(i\)th unit in the previous layer. In the diagram shown above, <strong>each arrow represents one of these weights</strong>. For example, the arrow going from \(a_2^{[1]}\) to \(a_3^{[2]}\) represents \(w_{23}^{[2]}\).</li><li>\(b_{j}^{[l]}\) is the bias term for unit \(j\) in layer \(l\). They play the same role as \(b\) in logistic regression. They are not represented in the figure.</li><li>\(g^{[l]}\) is the activation function for layer \(l\). For now, this is equivalent to the sigmoid function. We will see later that we can use other activation functions that work better than sigmoid for neural networks.</li></ul><p>\(w_{ij}^{[l]}\) and \(b_{j}^{[l]}\) are the <strong>learnable paremeters</strong> of the neural network: they can be trained by minimizing a cost function, as in logistic regression.</p><h3 id="matrix-notation">Matrix notation</h3><p>Do you like subscript notation? Neither I do. It turns out that all the computations in a neural network can be expressed as matrix operations. This simplifies notation a lot and makes implementations much faster, as computers prefer matrix operations to loops.</p><p>We can stack the \(z\) values and the activations in a single column vector per layer. If we do this, we get:</p>\[\boldsymbol z^{[l]} = \begin{bmatrix} z_1^{[l]} \\ z_2^{[l]} \\ ... \\ z_{n_l}^{[l]} \end{bmatrix} \text{ } \boldsymbol a^{[l]} = \begin{bmatrix} a_1^{[l]} \\ a_2^{[l]} \\ ... \\ a_{n_l}^{[l]} \end{bmatrix}\]<p>\(n_l\) represents the number of hidden units in layer \(l\). In the network above, \(n_1 = 4\), \(n_2 = 2\) and \(n_3 = 1\). The vectors \(\boldsymbol z^{[l]}\) and \(\boldsymbol a^{[l]}\) have dimensions \((n_l, 1)\).</p><p>We can also stack the biases into a vector and the weights into a matrix, defining:</p>\[\boldsymbol b^{[l]} = \begin{bmatrix} b_1^{[l]} \\ b_2^{[l]} \\ ... \\ b_{n_l}^{[l]} \end{bmatrix} \text{ } \boldsymbol W^{[l]} = \begin{bmatrix} w_{11}^{[l]} &amp; w_{12}^{[l]} &amp; ... &amp; w_{1n_{l-1}}^{[l]} \\ w_{21}^{[l]} &amp; w_{22}^{[l]} &amp; ... &amp; w_{2n_{l-1}}^{[l]} \\ ... &amp; ... &amp; ... &amp; ... \\ w_{n_l1}^{[l]} &amp; w_{n_l2}^{[l]} &amp; ... &amp; w_{n_ln_{l-1}}^{[l]} \end{bmatrix}\]<p>Where \(\boldsymbol b^{[l]}\) has dimensions \((n_l, 1)\), and \(\boldsymbol W^{[l]}\) is \((n_l, n_{l-1})\).</p><p>With this notation, all the subscripts in the previous equations go away:</p>\[\boldsymbol z^{[2]} = \boldsymbol W^{[2]} \boldsymbol a^{[1]} + \boldsymbol b^{[2]}\] \[\boldsymbol a^{[2]} = g^{[2]}(\boldsymbol z^{[2]})\]<p>Where \(g^{[2]}\) is the activation function for layer 2 (i.e. the sigmoid function), applied element-wise to \(\boldsymbol z^{[2]}\). So, instead of computing the activations unit by unit, we are now able to calculate the whole layer’s activations at the same time. Nice work!</p><h2 id="making-predictions">Making predictions</h2><p>We now have all in place to compute a prediction \(y_{prob}\) given the input features \(x_1, x_2, ..., x_n\). As with activations, let’s stack the input features into a column vector with dimensions \((n, 1)\):</p>\[\boldsymbol x = \boldsymbol a^{[0]} = \begin{bmatrix} x_1 \\ x_2 \\ ... \\ x_n \end{bmatrix}\]<p>We feed this vector \(\boldsymbol x\) to the first hidden layer (that’s I called it \(\boldsymbol a^{[0]}\)). For the network in the example, we would do:</p>\[\begin{align*} l &amp;= 1 &amp; \boldsymbol z^{[1]} &amp;= \boldsymbol W^{[1]} \boldsymbol a^{[0]} + \boldsymbol b^{[1]} &amp; \boldsymbol a^{[1]} &amp;= g^{[1]}(\boldsymbol z^{[1]}) \\ l &amp;= 2 &amp; \boldsymbol z^{[2]} &amp;= \boldsymbol W^{[2]} \boldsymbol a^{[1]} + \boldsymbol b^{[2]} &amp; \boldsymbol a^{[2]} &amp;= g^{[2]}(\boldsymbol z^{[2]}) \\ l &amp;= 3 &amp; \boldsymbol z^{[3]} &amp;= \boldsymbol W^{[3]} \boldsymbol a^{[2]} + \boldsymbol b^{[3]} &amp; \boldsymbol a^{[3]} &amp;= y_{prob} = g^{[3]}(\boldsymbol z^{[3]}) \end{align*}\]<p>Note that \(\boldsymbol a^{[3]}\) is really a real number instead of a vector, because we have just one hidden unit in the output layer. Also note that \(a^{[3]} = y_{prob} \in [0, 1]\), as \(g^{[3]}\) is the sigmoid function.</p><p>That’s it! Congratulations, you now know how neural networks make predictions!</p><h2 id="training-the-network">Training the network</h2><p>How can we learn the parameters \(\boldsymbol W^{[l]}\) and \(\boldsymbol b^{[l]}\)? Same as for other supervised learning problems: optimizing a cost function. If we are facing a binary classification problem, we can define the usual log loss function (if you are not familiar with it, check <a href="/kernel/posts/logistic-regression/#training-the-model">this</a>). For a single training example \(i\):</p>\[L(y^{(i)}, y_{prob}^{(i)}) = - y^{(i)}log(y_{prob}^{(i)}) - (1 - y^{(i)})log(1 - y_{prob}^{(i)})\]<p>The total cost will be the average over all training examples:</p>\[J(\text{all network parameters}) = \frac{1}{m} \sum_{i=1}^{m} L(y^{(i)}, y_{prob}^{(i)})\]<p>The cost \(J\) depends on all weight matrices \(\boldsymbol W^{[l]}\) and bias vectors \(\boldsymbol b^{[l]}\). It is thus a function of <em>a lot</em> of variables! Training the network is just an optimization problem: we have to figure out which are the values for the weights and biases that make \(J\) as small as possible.</p><p>I won’t go into the details of solving this optimization problem in this post, as it is quite involved. We will come back to it in a later post, where we will use <a href="https://keras.io/">Keras</a> to implement a neural network like the one presented here.</p><h2 id="activation-functions">Activation functions</h2><p>What is the role of activation functions? And why have I been writing \(g^{[l]}\) instead of just \(\sigma\)? Let’s find out.</p><p>Have you ever done manual feature engineering? If you have tried Kaggle’s <a href="https://www.kaggle.com/c/titanic">Titanic dataset</a> you may have found yourself creating features like “<em>Was this passenger a child?</em>”, “<em>Was this woman married?</em>”, given input features like <em>age</em> or <em>name</em>. The idea of neural networks is to let the model learn this feature engineering problem: the first layer learns to identify some low-level features, which are used by the next layer, and so on, until a prediction is output by the last layer.</p><p>Activation functions allow this feature-learning process by adding non-linearity to the neural network. They ensure that each unit is computing a non-linear function of the inputs. This is allows units in shallower layers to learn different features that can be used later.</p><p>What would happen if we did not use any activation function? Well, every single unit would end up computing a linear combination of its inputs. The final output would end up being a linear combination of the input features, no better than linear/logistic regression! Even with all those weights and bias terms! That would defy the original purpose of a neural network: being able to learn complex non-linear functions.</p><p>Thus, <strong>the activation function must be non-linear</strong>. The sigmoid function is non-linear, so it can be used as an activation function. But there are other options. Let me present you the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Rectified Linear Unit</a> (ReLU) function:</p>\[relu(z) = max\{0, z\}\]<p>It may be easier if we plot it:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/kernel/assets/img/neural-networks/relu.png" alt="ReLU" /></p><ul><li>If \(z &lt;= 0\), the output is zero.</li><li>If \(z &gt; 0\), the output is the same as the input.</li></ul><p>What is the advantage of the ReLU function versus the sigmoid? ReLU has mathematical properties that make the training process much faster. Concretely, it helps solve the <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">vanishing gradient problem</a>. If you are interested in this topic, <a href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/">this post</a> explores it further.</p><p>Note that ReLU is not bounded: if \(z\) becomes arbitrarily large, the output will be as big as the input. If we are dealing with binary classification, this makes it inadequate for the output layer, as the output unit should produce a number \(y_{prob} \in [0, 1]\) that can be interpreted as a probability.</p><p>How do we solve this? We will make each layer have its own activation function:</p><ul><li>Hidden layers (layers 1 and 2, in our example) will use ReLU: \(g^{[1]}(z) = g^{[2]}(z) = relu(z)\).</li><li>The output layer will keep using the sigmoid function: \(g^{[3]}(z) = \sigma(z)\).</li></ul><p>Following this scheme, we will keep getting output probabilities in the valid range while making the training process faster!</p><h2 id="conclusion">Conclusion</h2><p>That’s it for the basic of neural networks! We’ve covered what they are and how they make predictions. You know what hidden units, layers and activation functions are, which are the parameters of a network layer and how they are used to predict the outputs.</p><p>In the next post I will present the <a href="https://www.kaggle.com/c/digit-recognizer">MNIST dataset</a> and will explain how we can extend our neural network to handle <a href="https://en.wikipedia.org/wiki/Multiclass_classification">multiclass classification</a>.</p><p>I hope you enjoyed it! Feedback and suggestions are always welcome.</p><h2 id="references">References</h2><ul><li>Deep Learning Specialization, Coursera courses by Andrew Ng: https://www.coursera.org/specializations/deep-learning</li><li>When not to use Neural Networks, by Rahul Bhatia: <a href="https://medium.com/datadriveninvestor/when-not-to-use-neural-networks-89fb50622429">https://medium.com/datadriveninvestor/when-not-to-use-neural-networks-89fb50622429</a></li><li>A Gentle Introduction to the Rectified Linear Unit (ReLU), by Jason Brownlee: <a href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/">https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/</a></li><li>Sklearn documentation: <a href="https://scikit-learn.org/stable/">https://scikit-learn.org/stable/</a></li><li>XGBoost documentation: <a href="https://xgboost.readthedocs.io/en/latest/">https://xgboost.readthedocs.io/en/latest/</a></li></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/kernel/categories/data-science/'>Data Science</a>, <a href='/kernel/categories/machine-learning/'>Machine Learning</a>, <a href='/kernel/categories/deep-learning/'>Deep Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/kernel/tags/machinelearning/" class="post-tag no-text-decoration" >machinelearning</a> <a href="/kernel/tags/deeplearning/" class="post-tag no-text-decoration" >deeplearning</a> <a href="/kernel/tags/classification/" class="post-tag no-text-decoration" >classification</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><!-- Post sharing snippet v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://anarthal.github.io/kernel/posts/neural-networks/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank"> <i class="fa-fw fab fa-linkedin"></i> </a> <a href="https://twitter.com/intent/tweet?text=Deep dive into neural networks - the basics - Anarthal Kernel&url=https://anarthal.github.io/kernel/posts/neural-networks/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Deep dive into neural networks - the basics - Anarthal Kernel&u=https://anarthal.github.io/kernel/posts/neural-networks/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Deep dive into neural networks - the basics - Anarthal Kernel&url=https://anarthal.github.io/kernel/posts/neural-networks/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><!-- The Pannel on right side (Desktop views) v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><h3 data-toc-skip>Recent Update</h3><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/kernel/posts/underfitting-overfitting/">Underfitting, overfitting and model complexity</a></li><li><a href="/kernel/posts/logistic-regression/">Logistic regression</a></li></ul></div><div id="access-tags"><h3 data-toc-skip>Trending Tags</h3><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/kernel/tags/machinelearning/">machinelearning</a> <a class="post-tag" href="/kernel/tags/classification/">classification</a> <a class="post-tag" href="/kernel/tags/supervised/">supervised</a> <a class="post-tag" href="/kernel/tags/sklearn/">sklearn</a> <a class="post-tag" href="/kernel/tags/python/">python</a> <a class="post-tag" href="/kernel/tags/deeplearning/">deeplearning</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><h3 data-toc-skip class="pl-3 pt-2 mb-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="post-extend-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <!-- Navigation buttons at the bottom of the post. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --><div class="post-navigation d-flex justify-content-between"> <a href="/kernel/posts/underfitting-overfitting/" class="btn btn-outline-primary"><p>Underfitting, overfitting and model complexity</p></a> <span class="btn btn-outline-primary disabled"><p>-</p></span></div><!-- The Disqus lazy loading. Powered by: https://osvaldas.info/lazy-loading-disqus-comments v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung MIT License --><div id="disqus" class="pt-2 pb-4"><p class="font-italic text-center text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/kernel/assets/js/lib/jquery.disqusloader.min.js"></script> <script> var options = { scriptUrl: '//anarthal-kernel.disqus.com/embed.js', disqusConfig: function() { this.page.url = 'https://anarthal.github.io/kernel/posts/neural-networks/'; this.page.identifier = '/posts/neural-networks/'; } }; $.disqusLoader('#disqus', options); </script> <!-- Recommend the other 3 posts according to the tags and categories of the current post, if the number is not enough, use the other latest posts to supplement. v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div id="related-posts" class="mt-4 mb-2 mb-sm-4 pb-2"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/kernel/posts/logistic-regression/"><div class="card-body"> <span class="timeago small"> Jul 8, 2020 <i class="unloaded">2020-07-08T00:00:00+02:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Logistic regression</h3><div class="text-muted small"><p>In this post I will talk about one of the most basic models in Machine Learning: logistic regression. This post doesn’t assume any previous knowledge of logistic regression. Some prior knowledge on...</p></div></div></a></div><div class="card"> <a href="/kernel/posts/underfitting-overfitting/"><div class="card-body"> <span class="timeago small"> Jul 16, 2020 <i class="unloaded">2020-07-16T00:00:00+02:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Underfitting, overfitting and model complexity</h3><div class="text-muted small"><p>In this post I will talk about the underfitting and overfitting phenomena, and how model complexity affects them. I will also explain the bias-variance trade-off. We will demonstrate these concept...</p></div></div></a></div></div></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script> <!-- The Footer v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2020 <a href="https://github.com/anarthal">Ruben Perez</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/">Chirpy</a> theme.</p></div></div></footer></div><!-- The Search results v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/kernel/tags/machinelearning/">machinelearning</a> <a class="post-tag" href="/kernel/tags/classification/">classification</a> <a class="post-tag" href="/kernel/tags/supervised/">supervised</a> <a class="post-tag" href="/kernel/tags/sklearn/">sklearn</a> <a class="post-tag" href="/kernel/tags/python/">python</a> <a class="post-tag" href="/kernel/tags/deeplearning/">deeplearning</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <!-- The GA snippet v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-174570044-1', 'auto'); ga('send', 'pageview'); </script> <!-- Jekyll Simple Search loader v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/kernel/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://anarthal.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
